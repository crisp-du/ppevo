{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e290645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from allennlp_models.pretrained import load_predictor\n",
    "from transformers import XLNetTokenizer, XLNetModel\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4302c0f7",
   "metadata": {},
   "source": [
    "<h1>SRL Processor</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e921665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateSrlAnnotations(sentences):\n",
    "    \"\"\"\n",
    "    The function takes a list of sentences and returns a \n",
    "    DataFrame with Semantic Role Labelling.\n",
    "\n",
    "    The DataFrame will contain the following columns:\n",
    "    1) Sentences (sentences)\n",
    "    2) Semantic Frames (semantic_frames)\n",
    "    3) Verb associated with the semantic frame (verb)\n",
    "    4) Sentences labelled with SRL arguments (srl_labeled).\n",
    "    \"\"\"\n",
    "    return_data = {\n",
    "        'sentences' : list(),\n",
    "        'semantic_frames' : list(),\n",
    "        'verb' : list(),\n",
    "        'srl_labeled' : list()\n",
    "    }\n",
    "    srl_predictor = load_predictor(\"structured-prediction-srl-bert\")\n",
    "    for sentence in sentences:\n",
    "        srl_json = srl_predictor.predict(sentence)\n",
    "        \n",
    "        # 'verbs' contain the statements annotated with SRL for a given verb.\n",
    "        for verb_semantic_frame in srl_json['verbs']:\n",
    "            return_data['sentences'].append(sentence)\n",
    "            return_data['verb'].append(verb_semantic_frame['verb'])\n",
    "            return_data['srl_labeled'].append(verb_semantic_frame['description'])\n",
    "            \n",
    "            # 'words' contain tokens from the processed statement and 'tags' contain the assigned SRL \n",
    "            # tag for the word in the context of 'verb'.\n",
    "            processed_frame = [word for word,tag in \n",
    "                               zip(srl_json['words'],verb_semantic_frame['tags'])\n",
    "                              if tag!='O']\n",
    "            return_data['semantic_frames'].append(' '.join(processed_frame))\n",
    "    \n",
    "    return pd.DataFrame(return_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e3655",
   "metadata": {},
   "source": [
    "<h1> Semantic Frame Classifier </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd365f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_inputs(text_list, tokenizer, num_embeddings=512):\n",
    "    \"\"\"\n",
    "    Tokenizes and preprocesses a list of text inputs for a language model.\n",
    "\n",
    "    This function tokenizes each text string in the input list, truncating each \n",
    "    sequence to fit within the specified length limit. It appends special tokens \n",
    "    to denote the start and end of each sentence and pads sequences to ensure \n",
    "    consistent length across inputs.\n",
    "\n",
    "    Args:\n",
    "        text_list (list of str): A list of text strings to tokenize.\n",
    "        tokenizer: The tokenizer associated with the language model, used to \n",
    "            convert text to token IDs.\n",
    "        num_embeddings (int, optional): The maximum sequence length for the \n",
    "            tokenized output. Default is 512.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A padded array of token IDs for each input text, with shape \n",
    "        (len(text_list), num_embeddings).\n",
    "    \"\"\"\n",
    "    # tokenize the text, then truncate sequence to the desired length minus 2 for\n",
    "    # the 2 special characters\n",
    "    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t)[:num_embeddings-2], text_list))\n",
    "    # convert tokenized text into numeric ids for the appropriate LM\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    # append special token \"<s>\" and </s> to end of sentence\n",
    "    input_ids = [tokenizer.build_inputs_with_special_tokens(x) for x in input_ids]\n",
    "    # pad sequences\n",
    "    input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    return input_ids\n",
    "\n",
    "def create_attn_masks(input_ids):\n",
    "    \"\"\"\n",
    "    Generates attention masks for input sequences.\n",
    "\n",
    "    This function creates attention masks for each input sequence to indicate \n",
    "    which tokens should receive attention from the model. Tokens with a value of \n",
    "    1 indicate attention should be applied, while tokens with a value of 0 \n",
    "    (typically padding tokens) indicate that attention should be ignored.\n",
    "\n",
    "    Args:\n",
    "        input_ids (list of list of int): A list of token ID sequences, where each \n",
    "            sequence is a list of token IDs with padding tokens (value 0) as needed.\n",
    "\n",
    "    Returns:\n",
    "        list of list of float: A list of attention masks, with each mask matching \n",
    "        the structure of input_ids, where 1 indicates a token to be attended to \n",
    "        and 0 ignore.\n",
    "    \"\"\"\n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "\n",
    "    # Create a mask of 1s for each token followed by 0s for padding\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "    return attention_masks\n",
    "\n",
    "# Load the tokenizer.\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d261aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLNetForMultiLabelSequenceClassification(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    XLNet model for multi-label text classification tasks.\n",
    "\n",
    "    This model is built on top of the pretrained XLNet transformer and includes \n",
    "    a linear classification layer. It is designed for multi-label sequence \n",
    "    classification, where each input sequence can belong to multiple classes.\n",
    "\n",
    "    Args:\n",
    "        num_labels (int): The number of labels or classes for the classification task.\n",
    "\n",
    "    Attributes:\n",
    "        xlnet (XLNetModel): The base XLNet model for generating sequence representations.\n",
    "        classifier (torch.nn.Linear): A linear layer that maps the pooled hidden states\n",
    "            from the XLNet model to the output labels.\n",
    "\n",
    "    Methods:\n",
    "        forward(input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "            Computes the logits for each label for each sequence in the input batch.\n",
    "            If labels are provided, computes the multi-label classification loss.\n",
    "\n",
    "        pool_hidden_state(last_hidden_state):\n",
    "            Pools the hidden states from the last layer of the XLNet model to produce \n",
    "            a single mean vector for each sequence.\n",
    "\n",
    "    Returns:\n",
    "        If labels are provided, returns the computed BCEWithLogitsLoss.\n",
    "        Otherwise, returns the logits for each label for each sequence in the input batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels):\n",
    "        super(XLNetForMultiLabelSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.xlnet = XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "        self.classifier = torch.nn.Linear(768, num_labels)\n",
    "        torch.nn.init.xavier_normal_(self.classifier.weight)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None,\\\n",
    "              attention_mask=None, labels=None):\n",
    "        # last hidden layer\n",
    "        last_hidden_state = self.xlnet(input_ids=input_ids,\\\n",
    "                                       attention_mask=attention_mask,\\\n",
    "                                       token_type_ids=token_type_ids)\n",
    "        # pool the outputs into a mean vector\n",
    "        mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n",
    "        logits = self.classifier(mean_last_hidden_state)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels.float())\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "    def pool_hidden_state(self, last_hidden_state):\n",
    "        last_hidden_state = last_hidden_state[0]\n",
    "        mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n",
    "        return mean_last_hidden_state\n",
    "    \n",
    "def load_model(path):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained XLNetForMultiLabelSequenceClassification model from a checkpoint file.\n",
    "\n",
    "    This function loads the model state dictionary from a specified file path and \n",
    "    initializes an instance of XLNetForMultiLabelSequenceClassification with the \n",
    "    appropriate number of output labels based on the saved classifier weights.\n",
    "\n",
    "    Args:\n",
    "        path (str): The file path to the model checkpoint (typically a .bin file).\n",
    "    \n",
    "    Returns:\n",
    "        XLNetForMultiLabelSequenceClassification: An instance of the model with \n",
    "        weights loaded from the checkpoint file.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model_state_dict = checkpoint['state_dict']\n",
    "    model = XLNetForMultiLabelSequenceClassification(num_labels=model_state_dict[\"classifier.weight\"].size()[0])\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1968baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, statements, labels, device=\"cpu\", batch_size=8):\n",
    "    \"\"\"\n",
    "    Generates label predictions for a list of input statements using a trained \n",
    "    multi-label classification model.\n",
    "\n",
    "    This function takes a list of input statements, tokenizes them, and generates \n",
    "    batch-wise predictions by passing them through the model. It computes the \n",
    "    probability of each label using the model's output logits and returns the label \n",
    "    with the highest probability for each statement.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model used for generating predictions.\n",
    "        statements (list of str): List of text statements to classify.\n",
    "        labels (list of str): List of possible label names for classification.\n",
    "        device (str, optional): The device on which to run the model (\"cpu\" or \"cuda\"). \n",
    "            Default is \"cpu\".\n",
    "        batch_size (int, optional): The number of statements to process in each batch. \n",
    "            Default is 16.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Predicted label names for each input statement.\n",
    "    \"\"\"\n",
    "    num_iter = math.ceil(len(statements)/batch_size)\n",
    "    pred_probs = np.array([]).reshape(0, len(labels)) \n",
    "    # Move the model to GPU for prediction if available.\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        batch = statements[i*batch_size:(i+1)*batch_size]\n",
    "        input_ids = tokenize_inputs(batch, tokenizer, num_embeddings=250)\n",
    "        attention_masks = create_attn_masks(input_ids)\n",
    "        \n",
    "        input_ids = torch.tensor(input_ids).to(device)\n",
    "        attention_masks = torch.tensor(attention_masks, dtype=torch.long).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_masks)\n",
    "            logits = logits.sigmoid().detach().cpu().numpy()\n",
    "            pred_probs = np.vstack([pred_probs, logits])\n",
    "    \n",
    "    return [labels[pd.Series(prob).idxmax()] for prob in pred_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d412889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassifySemanticFrames(srl_dataframe):\n",
    "    \"\"\"\n",
    "    Classifies semantic frames in a DataFrame by appending sentence context and \n",
    "    using two levels of classifiers to assign categories.\n",
    "\n",
    "    This function combines each semantic frame with its associated sentence context \n",
    "    and performs multi-label classification using two pre-trained models:\n",
    "    - The first classifier distinguishes between \"skip\" and \"keep\".\n",
    "    - The second classifier distinguishes between \"policy categories\".\n",
    "\n",
    "    Args:\n",
    "        srl_dataframe (pd.DataFrame): A DataFrame containing columns 'semantic_frames' \n",
    "            and 'sentences', where 'semantic_frames' holds the semantic frame data \n",
    "            and 'sentences' provides contextual sentences.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with two additional columns:\n",
    "            - 'first_layer': Predictions from the first classifier (\"skip\" or \"keep\").\n",
    "            - 'second_layer': Predictions from the second classifier [FPCU, TPSC, UCC, UAED, DR].\n",
    "    \"\"\"\n",
    "    input_statements = [frames + '|||' + sentence for frames, sentence \n",
    "            in zip(srl_dataframe['semantic_frames'], srl_dataframe['sentences'])]\n",
    "\n",
    "    with open('models/skip_keep_labels.pkl', 'rb') as f:\n",
    "        skip_keep_labels = pickle.load(f)\n",
    "        \n",
    "    with open('models/policy_practice_labels.pkl', 'rb') as f:\n",
    "        policy_practice_labels = pickle.load(f)\n",
    "                        \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    srl_dataframe['first_layer'] = generate_predictions(load_model('models/skip_keep_classifier.bin'),\n",
    "                                                       input_statements, skip_keep_labels, device)\n",
    "    srl_dataframe['second_layer'] = generate_predictions(load_model('models/policy_practice_classifier.bin'),\n",
    "                                                       input_statements, policy_practice_labels, device)\n",
    "                        \n",
    "    return srl_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a9176",
   "metadata": {},
   "source": [
    "<h1> Privacy Specific Role Mapping </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "572f0788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_srl_labelled_statement(srl_labelled_statement):\n",
    "    \"\"\"\n",
    "    Parses a labeled SRL (Semantic Role Labeling) statement into a dictionary format.\n",
    "\n",
    "    This function extracts labeled segments from an SRL-annotated string, where each \n",
    "    segment is enclosed in brackets and has a \"label: value\" format. It converts \n",
    "    these segments into a dictionary with labels as keys and their corresponding \n",
    "    values as dictionary values.\n",
    "    \"\"\"\n",
    "    parsed_dict = {}\n",
    "\n",
    "    for entries in re.findall(r\"\\[(.+?)\\]\", srl_labelled_statement):\n",
    "        parsed_dict[str(entries.split(':')[0]).strip()] = str(entries.split(':')[-1]).strip()\n",
    "        \n",
    "    return parsed_dict\n",
    "\n",
    "def general_map(arg):\n",
    "    \"\"\"\n",
    "    Maps specific SRL argument roles to general categories.\n",
    "    \"\"\"\n",
    "    if arg == 'ARGM-LOC':\n",
    "        return 'LOCATION'\n",
    "    elif arg in ['ARGM-GOL','ARGM-PRP','ARGM-PNC']:\n",
    "        return 'PURPOSE'\n",
    "    elif arg == 'ARGM-MNR':\n",
    "        return 'MECHANISM'\n",
    "    elif arg == ['ARGM-TMP', 'ARGM-CAU', 'ARGM-ADV'] :\n",
    "        return 'TRIGGER'\n",
    "    elif arg == 'ARGM-MOD':\n",
    "        return 'MODAL'\n",
    "    elif arg == 'ARGM-NEG':\n",
    "        return 'NEGATION'\n",
    "    return None\n",
    "\n",
    "def map_srl_role_to_privacy_role(srl_dict, category):\n",
    "    \"\"\"\n",
    "    Maps SRL roles to privacy-specific roles using a pre-defined role mapping.\n",
    "\n",
    "    This function looks up a privacy role mapping for each SRL role found in the \n",
    "    input dictionary. It uses a category-specific role mapping and falls back on \n",
    "    general mappings if a category-specific mapping is not available.\n",
    "\n",
    "    Args:\n",
    "        srl_dict (dict): A dictionary with SRL role labels as keys and their \n",
    "            corresponding values.\n",
    "        category (str): The category used for privacy-specific role mapping.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping privacy roles to lists of values from the input.\n",
    "    \"\"\"\n",
    "    with open('annotations/privacy_specific_roles.json') as f:\n",
    "        verb_map = json.load(f)\n",
    "        \n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    privacy_role_map = dict()\n",
    "    # Do not process further if verb ('V') is not in the srl_dict.\n",
    "    \n",
    "    if 'V' not in srl_dict.keys(): return privacy_role_map\n",
    "    \n",
    "    # Category and verb specific role map.\n",
    "    specific_role_map = verb_map.get(\n",
    "        wnl.lemmatize(srl_dict['V'], 'v'), {}).get(category, {})\n",
    "    \n",
    "    for key, value in srl_dict.items():\n",
    "        # Propbank argument starting with 'C-' or 'R-' indicates\n",
    "        # multiple arguments of the same type.\n",
    "        if key.startswith('C-') or key.startswith('R-'):\n",
    "            key = key[2:]\n",
    "            \n",
    "        role = specific_role_map[key] if key in specific_role_map else general_map(key)\n",
    "        if not role: continue\n",
    "    \n",
    "        if role not in privacy_role_map:\n",
    "            privacy_role_map[role] = []\n",
    "        \n",
    "        privacy_role_map[role].append(value)\n",
    "        \n",
    "    return privacy_role_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c3030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddPrivacySpecificRoles(frame_classified_df):\n",
    "    \"\"\"\n",
    "    Adds privacy-specific roles to a DataFrame by mapping SRL roles to privacy roles.\n",
    "\n",
    "    This function filters the rows in the DataFrame that are tagged as 'Keep' in the \n",
    "    'first_layer' column, then applies the `map_srl_role_to_privacy_role` function to \n",
    "    map the SRL roles in each row to their corresponding privacy roles based on the \n",
    "    'second_layer' column. The resulting privacy role mappings are added as a new column \n",
    "    'privacy_role_map' in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        frame_classified_df (pd.DataFrame): A DataFrame with SRL labeled frames and \n",
    "            their classification results. It must contain the columns 'first_layer', \n",
    "            'second_layer', and 'srl_labeled'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with an additional column 'privacy_role_map' \n",
    "            containing the mapped privacy roles for each row.\n",
    "    \"\"\"\n",
    "    # Filter frames tagged as 'Skip'.\n",
    "    frame_classified_df = frame_classified_df[frame_classified_df['first_layer'] == 'Keep']\n",
    "    \n",
    "    privacy_roles = []\n",
    "    for _, rows in frame_classified_df.iterrows():\n",
    "        privacy_roles.append(map_srl_role_to_privacy_role(\n",
    "            parse_srl_labelled_statement(\n",
    "                rows['srl_labeled']), \n",
    "                rows['second_layer']))\n",
    "    \n",
    "    frame_classified_df['privacy_role_map'] = privacy_roles\n",
    "    return frame_classified_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8dd1a",
   "metadata": {},
   "source": [
    "<h1> PolicyPulse Pipeline Demonstration </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7a3c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = ['We collect the content, communications and other information you provide when you use our Products, including when you sign up for an account, create or share content, and message or communicate with others.',\n",
    "                   'We provide information and content to vendors and service providers who support our business , such as by providing technical infrastructure services , analyzing how our Products are used , providing customer service , facilitating payments or conducting surveys']\n",
    "srl_dataframe = CreateSrlAnnotations(input_sentences)\n",
    "classified_frame_dataframe = ClassifySemanticFrames(srl_dataframe)\n",
    "policypulse_output = AddPrivacySpecificRoles(classified_frame_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15322141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>semantic_frames</th>\n",
       "      <th>verb</th>\n",
       "      <th>srl_labeled</th>\n",
       "      <th>first_layer</th>\n",
       "      <th>second_layer</th>\n",
       "      <th>privacy_role_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We collect the content, communications and oth...</td>\n",
       "      <td>We collect the content , communications and ot...</td>\n",
       "      <td>collect</td>\n",
       "      <td>[ARG0: We] [V: collect] [ARG1: the content , c...</td>\n",
       "      <td>Keep</td>\n",
       "      <td>FPCU</td>\n",
       "      <td>{'FIRST_PARTY_ENTITY': ['We'], 'DATA': ['the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We provide information and content to vendors ...</td>\n",
       "      <td>We provide information and content to vendors ...</td>\n",
       "      <td>provide</td>\n",
       "      <td>[ARG0: We] [V: provide] [ARG1: information and...</td>\n",
       "      <td>Keep</td>\n",
       "      <td>TPSC</td>\n",
       "      <td>{'DATA': ['information and content', ', provid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences   \n",
       "0  We collect the content, communications and oth...  \\\n",
       "8  We provide information and content to vendors ...   \n",
       "\n",
       "                                     semantic_frames     verb   \n",
       "0  We collect the content , communications and ot...  collect  \\\n",
       "8  We provide information and content to vendors ...  provide   \n",
       "\n",
       "                                         srl_labeled first_layer second_layer   \n",
       "0  [ARG0: We] [V: collect] [ARG1: the content , c...        Keep         FPCU  \\\n",
       "8  [ARG0: We] [V: provide] [ARG1: information and...        Keep         TPSC   \n",
       "\n",
       "                                    privacy_role_map  \n",
       "0  {'FIRST_PARTY_ENTITY': ['We'], 'DATA': ['the c...  \n",
       "8  {'DATA': ['information and content', ', provid...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policypulse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a8601c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_env",
   "language": "python",
   "name": "bert_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
